---
title: "Convergence of Grandient Descent and its Variants"
collection: reports
permalink: /publication/gd-convergence
excerpt: 'In  this  text,  we  survey  prominent  Gradient  Descent  techniques  for optimization. Both, deterministic and stochastic methods are reviewed, such as SGD, Momentum, AdaGrad, ADAM and NAG. Convergence analyses of these algorithms are given, for objectives with various constraints on convexity, strong smoothness and strong convexity. Particularly for Adam, we review a recent work showing that the algorithm does not always converge, and restate the rigorous proof of the counterexample.  Finally, the text aims to act as a reference for the reader to refer to convergence analyses of the above-mentioned methods, along with certain comments on the performance of these methods.'
date: 2018-04-25
# venue: 'Journal 1'
paperurl: 'jaivardhankapoor.github.io/files/GD_review.pdf'
# citation: 'Your Name, You. (2009). &quot;Paper Title Number 1.&quot; <i>Journal 1</i>. 1(1).'
---
In  this  text,  we  survey  prominent  Gradient  Descent  techniques  for optimization. Both, deterministic and stochastic methods are reviewed, such as SGD, Momentum, AdaGrad, ADAM and NAG. Convergence analyses of these algorithms are given, for objectives with various constraints on convexity, strong smoothness and strong convexity. Particularly for Adam, we review a recent work showing that the algorithm does not always converge, and restate the rigorous proof of the counterexample.  Finally, the text aims to act as a reference for the reader to refer to convergence analyses of the above-mentioned methods, along with certain comments on the performance of these methods. [Link](jaivardhankapoor.github.io/files/GD_review.pdf)